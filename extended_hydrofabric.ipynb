{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b67663cc4d9593c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a771d372ed8b6f1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "huc_code = '14'\n",
    "huc_feature = 'huc2'\n",
    "feature_name = 'comid'\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "197f7fb2283876e5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def vcat(pattern, subset_dedup=None):\n",
    "    \"\"\"Read all CSVs matching pattern and vertically concatenate.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matched: {pattern}\")\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    if subset_dedup:\n",
    "        out = out.drop_duplicates(subset=subset_dedup)\n",
    "    return out\n",
    "\n",
    "def closest_segment_POD(point, gdf):\n",
    "    global fallback3_count, fallback3_ids\n",
    "    point_geom = point.geometry\n",
    "    point_gnis_id = point['SOURCE_GNIS_ID']\n",
    "    point_gnis_id_raw = str(point['Source GNIS ID Raw']).strip().lower()\n",
    "    point_source = str(point['WATER_SOURCE']).strip().lower()\n",
    "    pod_id = point.get('WDID', None)\n",
    "\n",
    "    def normalize_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return ''\n",
    "        name = name.lower().strip()\n",
    "\n",
    "        # Remove directional prefixes\n",
    "        directional_prefixes = ['east ', 'west ', 'north ', 'south ', 'upper ', 'lower ']\n",
    "        for p in directional_prefixes:\n",
    "            if name.startswith(p):\n",
    "                name = name[len(p):]\n",
    "\n",
    "        # Remove hydrologic suffixes\n",
    "        suffixes = [' river', ' creek', ' stream', ' wash', ' arroyo', ' fork', ' branch']\n",
    "        for s in suffixes:\n",
    "            if name.endswith(s):\n",
    "                name = name[:-len(s)]\n",
    "\n",
    "        return name.strip()\n",
    "\n",
    "    # Case 1: Use GNIS ID directly\n",
    "    if pd.notna(point_gnis_id) and point_gnis_id_raw != 'tbd':\n",
    "        matching_segments = gdf[gdf['gnis_id'] == point_gnis_id].copy()\n",
    "        if matching_segments.empty:\n",
    "            matching_segments = gdf.copy()\n",
    "        matching_segments['distance'] = matching_segments.distance(point_geom)\n",
    "        closest_idx = matching_segments['distance'].idxmin()\n",
    "        return matching_segments.loc[closest_idx]\n",
    "\n",
    "    # Case 2: Use normalized Water Source Name\n",
    "    elif point_source != 'tbd':\n",
    "\n",
    "        normalized_source = normalize_name(point_source)\n",
    "        candidate_segments = gdf[gdf['gnis_name'].notna()].copy()\n",
    "\n",
    "        def name_matches(gnis):\n",
    "            return normalize_name(gnis) == normalized_source\n",
    "\n",
    "        candidate_segments['name_match'] = candidate_segments['gnis_name'].apply(name_matches)\n",
    "        matched_segments = candidate_segments[candidate_segments['name_match']]\n",
    "\n",
    "        if matched_segments.empty:\n",
    "            matched_segments = gdf[gdf['streamorde'].fillna(0) >= 3].copy()\n",
    "            fallback3_count += 1\n",
    "            if pod_id is not None:\n",
    "                fallback3_ids.append(pod_id)\n",
    "\n",
    "        matched_segments = matched_segments.assign(\n",
    "            distance=matched_segments.geometry.distance(point_geom)\n",
    "        )\n",
    "        closest_idx = matched_segments['distance'].idxmin()\n",
    "        return matched_segments.loc[closest_idx]\n",
    "\n",
    "    # Case 3: Fallback to streamorde >= 3\n",
    "    else:\n",
    "        fallback_segments = gdf[gdf['streamorde'].fillna(0) >= 3].copy()\n",
    "        fallback3_count += 1\n",
    "        if pod_id is not None:\n",
    "            fallback3_ids.append(pod_id)\n",
    "\n",
    "        if fallback_segments.empty:\n",
    "            print(\"⚠️ Fallback segments are empty — using full GDF\")\n",
    "            fallback_segments = gdf.copy()\n",
    "        fallback_segments['distance'] = fallback_segments.distance(point_geom)\n",
    "        closest_idx = fallback_segments['distance'].idxmin()\n",
    "        return fallback_segments.loc[closest_idx]\n",
    "\n",
    "\n",
    "def closest_segment_func(point, gdf):\n",
    "    lake_segments = gdf[gdf['wbareatype'] == 'LakePond'].copy()\n",
    "    lake_segments.loc[:, 'distance'] = lake_segments.distance(point)\n",
    "    closest_idx = lake_segments['distance'].idxmin()\n",
    "    return lake_segments.loc[closest_idx]\n",
    "\n",
    "res_fallback_counter= 0\n",
    "def closest_segment_func2(point, gdf):\n",
    "    global res_fallback_counter\n",
    "    \"\"\"\n",
    "    Link a reservoir to the nearest LakePond segment, preferring name matches\n",
    "    on the 'RIVER' field when available. Robust to missing/None river names.\n",
    "    \"\"\"\n",
    "    # --- helpers ---\n",
    "    def safe_str(x):\n",
    "        return \"\" if pd.isna(x) or x is None else str(x)\n",
    "\n",
    "    def normalize_name(name):\n",
    "        name = safe_str(name).lower().strip()\n",
    "        if not name:\n",
    "            return name\n",
    "        # Remove common prefixes/suffixes and descriptors\n",
    "        prefixes = [\n",
    "            'east ', 'west ', 'north ', 'south ', 'upper ', 'lower ',\n",
    "            'middle fork of ', 'middle ', 'offstream ', 'off-stream ',\n",
    "        ]\n",
    "        for p in prefixes:\n",
    "            if name.startswith(p):\n",
    "                name = name[len(p):]\n",
    "        suffixes = [' river', ' creek', ' stream', ' wash', ' arroyo', ' fork', ' branch', ' canal']\n",
    "        for s in suffixes:\n",
    "            if name.endswith(s):\n",
    "                name = name[:-len(s)]\n",
    "        return name.strip()\n",
    "\n",
    "    river_name_norm = normalize_name(point.get('RIVER', ''))\n",
    "\n",
    "    # Candidate segments: lakes/ponds with a name\n",
    "    candidate_segments = gdf[\n",
    "        (gdf['wbareatype'] == 'LakePond') & (gdf['gnis_name'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    # If there is a usable river name, try a strict normalized match first\n",
    "    if river_name_norm:\n",
    "        candidate_segments = candidate_segments.assign(\n",
    "            _name_norm=candidate_segments['gnis_name'].apply(normalize_name)\n",
    "        )\n",
    "        matched = candidate_segments[candidate_segments['_name_norm'] == river_name_norm]\n",
    "\n",
    "        # If nothing strict-matched, try a softer contains match (handles things like \"Offstream Little Sandy Creek\")\n",
    "        if matched.empty:\n",
    "            matched = candidate_segments[candidate_segments['_name_norm'].str.contains(river_name_norm, na=False)]\n",
    "\n",
    "        # If there are any name-based matches, choose the nearest of those\n",
    "        if not matched.empty:\n",
    "            matched = matched.assign(distance=matched.geometry.distance(point.geometry))\n",
    "            return matched.loc[matched['distance'].idxmin()]\n",
    "\n",
    "    # Fallbacks (no/poor name or no matches):\n",
    "    # 1) Restrict spatially around the reservoir to avoid whole-basin scan\n",
    "    buf = gpd.GeoSeries([point.geometry], crs=gdf.crs).buffer(10_000).iloc[0]\n",
    "    spatial = gdf[(gdf['wbareatype'] == 'LakePond') & gdf.intersects(buf)].copy()\n",
    "    if not spatial.empty:\n",
    "        spatial = spatial.assign(distance=spatial.geometry.distance(point.geometry))\n",
    "        res_fallback_counter += 1\n",
    "        return spatial.loc[spatial['distance'].idxmin()]\n",
    "\n",
    "    # 2) Final fallback: nearest LakePond anywhere\n",
    "    lake_segments = gdf[gdf['wbareatype'] == 'LakePond'].copy()\n",
    "    if lake_segments.empty:\n",
    "        return None\n",
    "    lake_segments = lake_segments.assign(distance=lake_segments.geometry.distance(point.geometry))\n",
    "    return lake_segments.loc[lake_segments['distance'].idxmin()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c970b04f32d3ac4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Directories\n",
    "\n",
    "sites_df  = vcat(\"data/wade data/*/sites.csv\", subset_dedup=[\"SiteUUID\"])\n",
    "alloc_df  = vcat(\"data/wade data/*/waterallocations.csv\", subset_dedup= None)\n",
    "source_df = vcat(\"data/wade data/*/watersources.csv\", subset_dedup=[\"WaterSourceUUID\"])\n",
    "\n",
    "\n",
    "diversions_df = pd.read_csv('data/Diversions/diversion_points_lopez.csv')\n",
    "\n",
    "agg_csv= 'data/*' # If you have aggregated diversions with correct format (see example) and place them in data folder \n",
    "\n",
    "\n",
    "wbd_layer = gpd.read_file('data/wbd/WBDHU2.shp')\n",
    "\n",
    "# Load gage locations\n",
    "gage_shp = gpd.read_file('data/GageLoc/GageLoc.shp') # This is optional, it is nod added to the extended hydrofabric\n",
    "reference_fabric_path = 'data/reference fabric gpkg/reference_14.gpkg'\n",
    "flow_layer = gpd.read_file(reference_fabric_path, layer='reference_flowline')\n",
    "gage_data = gpd.read_file(reference_fabric_path, layer='event')\n",
    "# Load reservoir datasets\n",
    "resops_df = pd.read_csv('data/reservoirs/updated_reservoir_attributes.csv')\n",
    "grand_df  = pd.read_csv('data/reservoirs/GRAND.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d78e0bf3951b51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# WaDE data Harmonizing\n",
    "\n",
    "alloc_norm = alloc_df.copy()\n",
    "\n",
    "# split lists, explode, and clean whitespace/empties\n",
    "alloc_norm['SiteUUID'] = (\n",
    "    alloc_norm['SiteUUID']\n",
    "    .astype(str)\n",
    "    .str.split(',')\n",
    ")\n",
    "alloc_exploded = (\n",
    "    alloc_norm\n",
    "    .explode('SiteUUID')\n",
    "    .assign(SiteUUID=lambda d: d['SiteUUID'].str.strip())\n",
    ")\n",
    "alloc_exploded = alloc_exploded[alloc_exploded['SiteUUID'].notna() & (alloc_exploded['SiteUUID'] != '')]\n",
    "\n",
    "# --- Merge with sites (keep only allocations that actually have a site)\n",
    "wade_1 = pd.merge(alloc_exploded, sites_df, on='SiteUUID', how='inner')\n",
    "\n",
    "\n",
    "if 'WaterSourceUUIDs' in wade_1.columns:\n",
    "    wade_1['WaterSourceUUIDs'] = (\n",
    "        wade_1['WaterSourceUUIDs']\n",
    "        .astype(str)\n",
    "        .str.split(',')\n",
    "    )\n",
    "    wade_1 = (\n",
    "        wade_1\n",
    "        .explode('WaterSourceUUIDs')\n",
    "        .assign(WaterSourceUUID=lambda d: d['WaterSourceUUIDs'].str.strip())\n",
    "        .drop(columns=['WaterSourceUUIDs'])\n",
    "    )\n",
    "elif 'WaterSourceUUID' in wade_1.columns:\n",
    "    # Ensure clean strings if it's already singular\n",
    "    wade_1['WaterSourceUUID'] = wade_1['WaterSourceUUID'].astype(str).str.strip()\n",
    "\n",
    "if 'SiteNativeID' in wade_1.columns:\n",
    "    wade_1 = wade_1.rename(columns={'SiteNativeID': 'WDID'})\n",
    "\n",
    "wade_df = pd.merge(wade_1, source_df, on='WaterSourceUUID', how='left')\n",
    "\n",
    "wade_df['WDID'] = wade_df['WDID'].astype(str)\n",
    "diversions_df['WadeID'] = diversions_df['WadeID'].astype(str)\n",
    "wade_df['WDID'] = wade_df['WDID'].str.strip()\n",
    "diversions_df['WadeID'] = diversions_df['WadeID'].str.strip()\n",
    "if os.path.exists(agg_csv):\n",
    "    aggregated_diversions_df = pd.read_csv(agg_csv)\n",
    "    aggregated_diversions_df['Aggregation ID'] = aggregated_diversions_df['Aggregation ID'].astype(str).str.strip()\n",
    "else:\n",
    "    print(\" aggregated_table.csv not found. Continuing without aggregated diversion data.\")\n",
    "    # Make empty dataframe with expected columns so code doesn't break\n",
    "    aggregated_diversions_df = pd.DataFrame(columns=[\n",
    "        'Aggregation ID', 'Aggregation Name', 'Water Source'\n",
    "    ])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d119a391318f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Diversions Layer\n",
    "\n",
    "diversions_df['geometry'] = [Point(xy) for xy in zip(diversions_df['X'], diversions_df['Y'])]\n",
    "diversions_gdf = gpd.GeoDataFrame(diversions_df, geometry='geometry', crs='EPSG:4326')  # Assuming input is in lat/lon\n",
    "\n",
    "diversions_gdf = diversions_gdf.to_crs(wbd_layer.crs)\n",
    "\n",
    "wbd_selected = wbd_layer[wbd_layer[huc_feature] == huc_code]\n",
    "\n",
    "diversions_selected = gpd.clip(diversions_gdf, wbd_selected)\n",
    "\n",
    "diversions_selected = diversions_selected.drop(columns=['geometry', 'index_right'], errors='ignore').copy()\n",
    "POD_df = diversions_selected.copy().drop(columns=['ID', 'State'], errors='')\n",
    "POD_df.columns = ['WDID', 'LATITUDE', 'LONGITUDE','POI_NATIVE_ID']\n",
    "\n",
    "POD_df['TYPE'] = None\n",
    "POD_df['BENEFICIAL_CATEGORY_USE'] = 'TBD'\n",
    "POD_df['SITE_NAME'] ='TBD'\n",
    "POD_df['WATER_SOURCE'] ='TBD'\n",
    "POD_df['SOURCE_GNIS_ID']='TBD'\n",
    "\n",
    "\n",
    "wade_wdids = set(wade_df['WDID'].values)\n",
    "agg_wdids  = set(aggregated_diversions_df['Aggregation ID'].values)\n",
    "\n",
    "# Loop using itertuples\n",
    "for row in tqdm(POD_df.itertuples(), total=len(POD_df), desc=\"Classifying PODs\"):\n",
    "\n",
    "    idx = row.Index\n",
    "    diversion_id = row.WDID\n",
    "\n",
    "    # Case 1 — Physical diversion\n",
    "    if diversion_id in wade_wdids:\n",
    "        match = wade_df.loc[wade_df['WDID'] == diversion_id].iloc[0]\n",
    "\n",
    "        POD_df.at[idx, 'TYPE'] = 'Physical'\n",
    "        POD_df.at[idx, 'SITE_NAME'] = match['SiteName']\n",
    "        POD_df.at[idx, 'WATER_SOURCE'] = match['WaterSourceName']\n",
    "        POD_df.at[idx, 'BENEFICIAL_CATEGORY_USE'] = match['BeneficialUseCategory']\n",
    "\n",
    "    # Case 2 — Aggregated diversion\n",
    "    elif diversion_id in agg_wdids:\n",
    "        match = aggregated_diversions_df.loc[\n",
    "            aggregated_diversions_df['Aggregation ID'] == diversion_id\n",
    "        ].iloc[0]\n",
    "\n",
    "        POD_df.at[idx, 'TYPE'] = 'Aggregated Diversion'\n",
    "        POD_df.at[idx, 'SITE_NAME'] = match['Aggregation Name']\n",
    "        POD_df.at[idx, 'WATER_SOURCE'] = match['Water Source']\n",
    "        \n",
    "POD_df = POD_df.dropna(subset=['TYPE'])  \n",
    "POD_df.to_csv(f\"data/output/PODtable_{huc_code}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1ca4121d8ab558",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --- POI Extraction Block ---\n",
    "\n",
    "# Load POD table generated earlier in the notebook\n",
    "POD_df = pd.read_csv(f'data/output/PODtable_{huc_code}.csv', dtype={'WDID': str})\n",
    "\n",
    "# Filter to selected HUC2\n",
    "huc_boundary = wbd_layer[wbd_layer[huc_feature] == huc_code]\n",
    "\n",
    "\n",
    "# Merge reservoir attributes\n",
    "resops_updated = resops_df.merge(\n",
    "    grand_df.rename(columns={'GRAND_ID': 'DAM_ID'})[\n",
    "        ['DAM_ID', 'RIVER', 'CAP_MCM', 'DAM_HGT_M', 'AREA_SKM', 'MAIN_USE']\n",
    "    ],\n",
    "    on='DAM_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert POD table to GDF\n",
    "pod_gdf = gpd.GeoDataFrame(\n",
    "    POD_df,\n",
    "    geometry=gpd.points_from_xy(POD_df.LONGITUDE, POD_df.LATITUDE),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Convert reservoir CSV to GDF\n",
    "resops_gdf = gpd.GeoDataFrame(\n",
    "    resops_updated,\n",
    "    geometry=gpd.points_from_xy(resops_updated.LONGITUDE, resops_updated.LATITUDE),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Reproject everything to match WBD layer\n",
    "target_crs = huc_boundary.crs\n",
    "\n",
    "pod_gdf    = pod_gdf.to_crs(target_crs)\n",
    "resops_gdf = resops_gdf.to_crs(target_crs)\n",
    "gage_shp   = gage_shp.to_crs(target_crs)\n",
    "huc_boundary = huc_boundary.to_crs(target_crs)\n",
    "\n",
    "# Select points inside the HUC boundary\n",
    "pod_selected    = pod_gdf[pod_gdf.within(huc_boundary.unary_union)]\n",
    "resops_selected = resops_gdf[resops_gdf.within(huc_boundary.unary_union)]\n",
    "gage_selected   = gage_shp[gage_shp.within(huc_boundary.unary_union)]\n",
    "\n",
    "# Output file\n",
    "output_gpkg_path = f\"data/output/POI_{huc_code}.gpkg\"\n",
    "\n",
    "# Write layers\n",
    "pod_selected.to_file(output_gpkg_path, layer=\"DIVERSION_POINTS\", driver=\"GPKG\")\n",
    "resops_selected.to_file(output_gpkg_path, layer=\"RESERVOIR_POINTS\", driver=\"GPKG\")\n",
    "gage_selected.to_file(output_gpkg_path, layer=\"GAGE_POINTS\", driver=\"GPKG\")\n",
    "\n",
    "print(f\"Selected POI layers saved to: {output_gpkg_path}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "643ea749cbc5ffb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "POD_data = gpd.read_file(f'data/output/POI_{huc_code}.gpkg', layer='DIVERSION_POINTS')\n",
    "res_data = gpd.read_file(f'data/output/POI_{huc_code}.gpkg', layer='RESERVOIR_POINTS')\n",
    "\n",
    "\n",
    "target_crs = flow_layer.crs\n",
    "POD_data = POD_data.to_crs(target_crs)\n",
    "res_data = res_data.to_crs(target_crs)\n",
    "flow_layer['gnis_id'] = pd.to_numeric(flow_layer['gnis_id'], errors='coerce')\n",
    "flow_layer['streamorde'] = pd.to_numeric(flow_layer['streamorde'], errors='coerce')\n",
    "POD_data['Source GNIS ID Raw'] = POD_data['SOURCE_GNIS_ID']\n",
    "\n",
    "fallback3_count = 0\n",
    "fallback3_ids = []\n",
    "closest_features = []\n",
    "updated_gnis_ids = []\n",
    "\n",
    "\n",
    "for idx, point in tqdm(POD_data.iterrows(), total=len(POD_data), desc=\"Linking PODs IDs\"):\n",
    "    closest_segment = closest_segment_POD(point, flow_layer)\n",
    "    closest_feature_value = closest_segment[feature_name] if closest_segment is not None else None\n",
    "    POD_data.at[idx, 'WATER_SOURCE'] = closest_segment['gnis_name']\n",
    "    POD_data.at[idx, 'SOURCE_GNIS_ID'] = closest_segment['gnis_id']\n",
    "    closest_features.append(closest_feature_value)\n",
    "\n",
    "debug_point = POD_data.iloc[11]\n",
    "debug_geom = debug_point.geometry\n",
    "debug_gnis_id = debug_point['SOURCE_GNIS_ID']\n",
    "matching_segments = flow_layer[flow_layer['gnis_id'] == debug_gnis_id].copy()\n",
    "debug_closest_segment = closest_segment_POD(debug_point, flow_layer)\n",
    "\n",
    "POD_data['SOURCE_COMID'] = closest_features\n",
    "#POD_data['POI_NativeID'] = POD_data['WDID']\n",
    "POD_data = POD_data.drop(columns='Source GNIS ID Raw')\n",
    "closest_features_res = []\n",
    "\n",
    "for idx, point in tqdm(res_data.iterrows(),total=len(res_data), desc=\"Linking Reservoir IDs\") :\n",
    "    river_name = res_data.at[idx, 'RIVER']\n",
    "    point_geom = point.geometry\n",
    "    closest_segment = closest_segment_func2(point, flow_layer)\n",
    "    closest_feature_value = closest_segment[feature_name] if closest_segment is not None else None\n",
    "    closest_features_res.append(closest_feature_value)\n",
    "\n",
    "res_data['SOURCE_COMID'] = closest_features_res\n",
    "res_data['POI_NATIVE_ID'] = res_data['NID_ID']\n",
    "\n",
    "#gage_data['SOURCE_COMID'] = np.nan\n",
    "#gage_data.loc[gage_data['hl_reference'] == 'type_gages', 'Source_' + feature_name] = gage_data['hy_id']\n",
    "\n",
    "#gage_data['POI_NativeID'] = np.nan\n",
    "#gage_data.loc[gage_data['hl_reference'] == 'type_gages', 'POI_NativeID'] = gage_data['hl_link']\n",
    "\n",
    "print(f\"\\nNumber of PODs that used fallback 3: {fallback3_count}\")\n",
    "print(f\"\\nNumber of Reservoirs that used fallback 3: {res_fallback_counter}\")\n",
    "\n",
    "enhanced_fabric_path = f'data/output/enhanced_reference_{huc_code}.gpkg'\n",
    "shutil.copyfile(reference_fabric_path, enhanced_fabric_path)\n",
    "\n",
    "# with fiona.Env():\n",
    "#     fiona.remove(enhanced_fabric_path, layer='event')\n",
    "\n",
    "#gage_data.to_file(enhanced_fabric_path, layer='event', driver='GPKG', mode='a')\n",
    "POD_data.to_file(enhanced_fabric_path, layer='DIVERSION_POINTS', driver='GPKG', mode='a')\n",
    "res_data.to_file(enhanced_fabric_path, layer='RESERVOIR_POINTS', driver='GPKG', mode='a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e852b964afdf72",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4f4ab87bfb4556de",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
