{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b67663cc4d9593c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import importlib\n",
    "import extended_hydrofabric_functions as hf\n",
    "importlib.reload(hf)\n",
    "from extended_hydrofabric_functions import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a771d372ed8b6f1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "huc_code = '14'\n",
    "huc_feature = 'huc2'\n",
    "feature_name = 'comid'\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "197f7fb2283876e5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Functions\n",
    "# (moved to external script: extended_hydrofabric_functions.py)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c970b04f32d3ac4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Directories\n",
    "\n",
    "sites_df  = vcat(\"data/wade data/*/sites.csv\", subset_dedup=[\"SiteUUID\"])\n",
    "alloc_df  = vcat(\"data/wade data/*/waterallocations.csv\", subset_dedup= None)\n",
    "source_df = vcat(\"data/wade data/*/watersources.csv\", subset_dedup=[\"WaterSourceUUID\"])\n",
    "\n",
    "\n",
    "diversions_df = pd.read_csv('data/Diversions/diversion_points_lopez.csv')\n",
    "\n",
    "agg_csv= 'data/*' # If you have aggregated diversions with correct format (see example) and place them in data folder \n",
    "\n",
    "\n",
    "wbd_layer = gpd.read_file('data/wbd/WBDHU2.shp')\n",
    "\n",
    "# Load gage locations\n",
    "gage_shp = gpd.read_file('data/GageLoc/GageLoc.shp') # This is optional, it is nod added to the extended hydrofabric\n",
    "reference_fabric_path = 'data/reference fabric gpkg/reference_14.gpkg'\n",
    "flow_layer = gpd.read_file(reference_fabric_path, layer='reference_flowline')\n",
    "gage_data = gpd.read_file(reference_fabric_path, layer='event')\n",
    "# Load reservoir datasets\n",
    "resops_df = pd.read_csv('data/reservoirs/updated_reservoir_attributes.csv')\n",
    "grand_df  = pd.read_csv('data/reservoirs/GRAND.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d78e0bf3951b51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# WaDE data Harmonizing\n",
    "\n",
    "alloc_norm = alloc_df.copy()\n",
    "\n",
    "# split lists, explode, and clean whitespace/empties\n",
    "alloc_norm['SiteUUID'] = (\n",
    "    alloc_norm['SiteUUID']\n",
    "    .astype(str)\n",
    "    .str.split(',')\n",
    ")\n",
    "alloc_exploded = (\n",
    "    alloc_norm\n",
    "    .explode('SiteUUID')\n",
    "    .assign(SiteUUID=lambda d: d['SiteUUID'].str.strip())\n",
    ")\n",
    "alloc_exploded = alloc_exploded[alloc_exploded['SiteUUID'].notna() & (alloc_exploded['SiteUUID'] != '')]\n",
    "\n",
    "# --- Merge with sites (keep only allocations that actually have a site)\n",
    "wade_1 = pd.merge(alloc_exploded, sites_df, on='SiteUUID', how='inner')\n",
    "\n",
    "\n",
    "if 'WaterSourceUUIDs' in wade_1.columns:\n",
    "    wade_1['WaterSourceUUIDs'] = (\n",
    "        wade_1['WaterSourceUUIDs']\n",
    "        .astype(str)\n",
    "        .str.split(',')\n",
    "    )\n",
    "    wade_1 = (\n",
    "        wade_1\n",
    "        .explode('WaterSourceUUIDs')\n",
    "        .assign(WaterSourceUUID=lambda d: d['WaterSourceUUIDs'].str.strip())\n",
    "        .drop(columns=['WaterSourceUUIDs'])\n",
    "    )\n",
    "elif 'WaterSourceUUID' in wade_1.columns:\n",
    "    # Ensure clean strings if it's already singular\n",
    "    wade_1['WaterSourceUUID'] = wade_1['WaterSourceUUID'].astype(str).str.strip()\n",
    "\n",
    "if 'SiteNativeID' in wade_1.columns:\n",
    "    wade_1 = wade_1.rename(columns={'SiteNativeID': 'WDID'})\n",
    "\n",
    "wade_df = pd.merge(wade_1, source_df, on='WaterSourceUUID', how='left')\n",
    "\n",
    "wade_df['WDID'] = wade_df['WDID'].astype(str)\n",
    "diversions_df['WadeID'] = diversions_df['WadeID'].astype(str)\n",
    "wade_df['WDID'] = wade_df['WDID'].str.strip()\n",
    "diversions_df['WadeID'] = diversions_df['WadeID'].str.strip()\n",
    "if os.path.exists(agg_csv):\n",
    "    aggregated_diversions_df = pd.read_csv(agg_csv)\n",
    "    aggregated_diversions_df['Aggregation ID'] = aggregated_diversions_df['Aggregation ID'].astype(str).str.strip()\n",
    "else:\n",
    "    print(\" aggregated_table.csv not found. Continuing without aggregated diversion data.\")\n",
    "\n",
    "    aggregated_diversions_df = pd.DataFrame(columns=[\n",
    "        'Aggregation ID', 'Aggregation Name', 'Water Source'\n",
    "    ])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d119a391318f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Diversions Layer\n",
    "\n",
    "diversions_df['geometry'] = [Point(xy) for xy in zip(diversions_df['X'], diversions_df['Y'])]\n",
    "diversions_gdf = gpd.GeoDataFrame(diversions_df, geometry='geometry', crs='EPSG:4326')  # Assuming input is in lat/lon\n",
    "\n",
    "diversions_gdf = diversions_gdf.to_crs(wbd_layer.crs)\n",
    "\n",
    "wbd_selected = wbd_layer[wbd_layer[huc_feature] == huc_code]\n",
    "\n",
    "diversions_selected = gpd.clip(diversions_gdf, wbd_selected)\n",
    "\n",
    "diversions_selected = diversions_selected.drop(columns=['geometry', 'index_right'], errors='ignore').copy()\n",
    "POD_df = diversions_selected.copy().drop(columns=['ID', 'State'], errors='ignore')\n",
    "POD_df.columns = ['WDID', 'LATITUDE', 'LONGITUDE', 'POI_NATIVE_ID']\n",
    "\n",
    "# Ensure string keys (important for merges)\n",
    "POD_df['WDID'] = POD_df['WDID'].astype(str).str.strip()\n",
    "wade_df['WDID'] = wade_df['WDID'].astype(str).str.strip()\n",
    "aggregated_diversions_df['Aggregation ID'] = aggregated_diversions_df['Aggregation ID'].astype(str).str.strip()\n",
    "\n",
    "# ---- Join 1: WaDE (Physical) ----\n",
    "wade_cols = ['WDID', 'SiteName', 'WaterSourceName', 'BeneficialUseCategory']\n",
    "wade_keyed = wade_df[wade_cols].drop_duplicates('WDID')\n",
    "\n",
    "pod_wade = POD_df.merge(\n",
    "    wade_keyed,\n",
    "    on='WDID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---- Join 2: Aggregated diversions ----\n",
    "agg_cols = ['Aggregation ID', 'Aggregation Name', 'Water Source']\n",
    "agg_keyed = aggregated_diversions_df[agg_cols].drop_duplicates('Aggregation ID')\n",
    "\n",
    "pod_all = pod_wade.merge(\n",
    "    agg_keyed,\n",
    "    left_on='WDID',\n",
    "    right_on='Aggregation ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "is_physical = pod_all['SiteName'].notna()\n",
    "is_agg = (~is_physical) & pod_all['Aggregation Name'].notna()\n",
    "\n",
    "pod_all['TYPE'] = np.select(\n",
    "    [is_physical, is_agg],\n",
    "    ['Physical', 'Aggregated Diversion'],\n",
    "    default=None\n",
    ")\n",
    "\n",
    "# Fill outputs based on which tier matched\n",
    "pod_all['SITE_NAME'] = np.where(is_physical, pod_all['SiteName'], pod_all['Aggregation Name'])\n",
    "pod_all['WATER_SOURCE'] = np.where(is_physical, pod_all['WaterSourceName'], pod_all['Water Source'])\n",
    "pod_all['BENEFICIAL_CATEGORY_USE'] = np.where(is_physical, pod_all['BeneficialUseCategory'], 'TBD')\n",
    "\n",
    "\n",
    "pod_all['SOURCE_GNIS_ID'] = 'TBD'\n",
    "\n",
    "# Keep only classified\n",
    "POD_df = pod_all.dropna(subset=['TYPE']).copy()\n",
    "\n",
    "# Drop helper/join columns\n",
    "POD_df = POD_df.drop(\n",
    "    columns=[\n",
    "        'SiteName','WaterSourceName','BeneficialUseCategory',\n",
    "        'Aggregation ID','Aggregation Name','Water Source'\n",
    "    ],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "POD_df.to_csv(f\"data/output/PODtable_{huc_code}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1ca4121d8ab558",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --- POI Extraction Block ---\n",
    "\n",
    "# Load POD table generated earlier in the notebook\n",
    "POD_df = pd.read_csv(f'data/output/PODtable_{huc_code}.csv', dtype={'WDID': str})\n",
    "\n",
    "# Filter to selected HUC2\n",
    "huc_boundary = wbd_layer[wbd_layer[huc_feature] == huc_code]\n",
    "\n",
    "\n",
    "# Merge reservoir attributes\n",
    "resops_updated = resops_df.merge(\n",
    "    grand_df.rename(columns={'GRAND_ID': 'DAM_ID'})[\n",
    "        ['DAM_ID', 'RIVER', 'CAP_MCM', 'DAM_HGT_M', 'AREA_SKM', 'MAIN_USE']\n",
    "    ],\n",
    "    on='DAM_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert POD table to GDF\n",
    "pod_gdf = gpd.GeoDataFrame(\n",
    "    POD_df,\n",
    "    geometry=gpd.points_from_xy(POD_df.LONGITUDE, POD_df.LATITUDE),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Convert reservoir CSV to GDF\n",
    "resops_gdf = gpd.GeoDataFrame(\n",
    "    resops_updated,\n",
    "    geometry=gpd.points_from_xy(resops_updated.LONGITUDE, resops_updated.LATITUDE),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Reproject everything to match WBD layer\n",
    "target_crs = huc_boundary.crs\n",
    "\n",
    "pod_gdf    = pod_gdf.to_crs(target_crs)\n",
    "resops_gdf = resops_gdf.to_crs(target_crs)\n",
    "gage_shp   = gage_shp.to_crs(target_crs)\n",
    "huc_boundary = huc_boundary.to_crs(target_crs)\n",
    "\n",
    "# Select points inside the HUC boundary\n",
    "pod_selected    = pod_gdf[pod_gdf.within(huc_boundary.unary_union)]\n",
    "resops_selected = resops_gdf[resops_gdf.within(huc_boundary.unary_union)]\n",
    "gage_selected   = gage_shp[gage_shp.within(huc_boundary.unary_union)]\n",
    "\n",
    "# Output file\n",
    "output_gpkg_path = f\"data/output/POI_{huc_code}.gpkg\"\n",
    "\n",
    "# Write layers\n",
    "pod_selected.to_file(output_gpkg_path, layer=\"DIVERSION_POINTS\", driver=\"GPKG\")\n",
    "resops_selected.to_file(output_gpkg_path, layer=\"RESERVOIR_POINTS\", driver=\"GPKG\")\n",
    "gage_selected.to_file(output_gpkg_path, layer=\"GAGE_POINTS\", driver=\"GPKG\")\n",
    "\n",
    "print(f\"Selected POI layers saved to: {output_gpkg_path}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "643ea749cbc5ffb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "POD_data = gpd.read_file(f'data/output/POI_{huc_code}.gpkg', layer='DIVERSION_POINTS')\n",
    "res_data = gpd.read_file(f'data/output/POI_{huc_code}.gpkg', layer='RESERVOIR_POINTS')\n",
    "\n",
    "\n",
    "target_crs = flow_layer.crs\n",
    "POD_data = POD_data.to_crs(target_crs)\n",
    "res_data = res_data.to_crs(target_crs)\n",
    "flow_layer['gnis_id'] = pd.to_numeric(flow_layer['gnis_id'], errors='coerce')\n",
    "flow_layer['streamorde'] = pd.to_numeric(flow_layer['streamorde'], errors='coerce')\n",
    "POD_data['Source GNIS ID Raw'] = POD_data['SOURCE_GNIS_ID']\n",
    "\n",
    "flow = flow_layer.copy()\n",
    "flow[\"gnis_id\"] = pd.to_numeric(flow[\"gnis_id\"], errors=\"coerce\")\n",
    "flow[\"streamorde\"] = pd.to_numeric(flow[\"streamorde\"], errors=\"coerce\")\n",
    "\n",
    "pods = POD_data.copy()\n",
    "pods[\"WDID\"] = pods[\"WDID\"].astype(str)\n",
    "\n",
    "pods[\"Source GNIS ID Raw\"] = pods[\"SOURCE_GNIS_ID\"]\n",
    "\n",
    "# Numeric GNIS id for join\n",
    "pods[\"SOURCE_GNIS_ID_num\"] = pd.to_numeric(pods[\"SOURCE_GNIS_ID\"], errors=\"coerce\")\n",
    "flow[\"gnis_id_num\"] = flow[\"gnis_id\"]\n",
    "\n",
    "# Normalized names for Tier 2 join\n",
    "pods[\"WATER_SOURCE_str\"] = pods[\"WATER_SOURCE\"].astype(str).str.strip()\n",
    "pods[\"ws_norm\"] = pods[\"WATER_SOURCE_str\"].apply(lambda x: normalize_name(x) if x.lower() != \"tbd\" else \"\")\n",
    "flow[\"gnis_name_norm\"] = flow[\"gnis_name\"].astype(str).apply(normalize_name)\n",
    "\n",
    "# Results containers \n",
    "res_comid = {}\n",
    "res_gnis_name = {}\n",
    "res_gnis_id = {}\n",
    "\n",
    "# Counters \n",
    "res_fallback_counter = 0\n",
    "fallback3_count = 0\n",
    "fallback4_count = 0\n",
    "fallback3_ids = []\n",
    "no_link_count = 0\n",
    "no_link_ids = []\n",
    "no_link_records = []\n",
    "\n",
    "feature_name = \"comid\"  # you set above\n",
    "\n",
    "# -------------------------\n",
    "# Tier 1: GNIS ID join\n",
    "# if SOURCE_GNIS_ID is not na and Source GNIS ID Raw != 'tbd'\n",
    "# -------------------------\n",
    "t1_mask = pods[\"SOURCE_GNIS_ID_num\"].notna() & (pods[\"Source GNIS ID Raw\"].astype(str).str.strip().str.lower() != \"tbd\")\n",
    "pods_t1 = pods.loc[t1_mask, [\"WDID\", \"geometry\", \"SOURCE_GNIS_ID_num\"]].rename(columns={\"geometry\": \"geom_pod\"}).copy()\n",
    "\n",
    "if not pods_t1.empty:\n",
    "    cand_t1 = pods_t1.merge(\n",
    "        flow[[\"comid\", \"gnis_id\", \"gnis_id_num\", \"gnis_name\", \"geometry\"]].rename(columns={\"geometry\": \"geom_seg\"}),\n",
    "        left_on=\"SOURCE_GNIS_ID_num\",\n",
    "        right_on=\"gnis_id_num\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Matched GNIS rows (have comid)\n",
    "    matched = cand_t1[cand_t1[\"comid\"].notna()].copy()\n",
    "    if not matched.empty:\n",
    "        matched[\"dist\"] = matched[\"geom_seg\"].distance(matched[\"geom_pod\"])\n",
    "        best_idx = matched.groupby(\"WDID\")[\"dist\"].idxmin()\n",
    "        best = matched.loc[best_idx]\n",
    "\n",
    "        for r in best.itertuples(index=False):\n",
    "            res_comid[r.WDID] = int(r.comid) if pd.notna(r.comid) else None\n",
    "            res_gnis_name[r.WDID] = r.gnis_name\n",
    "            res_gnis_id[r.WDID] = r.gnis_id\n",
    "\n",
    "    # Unmatched GNIS (no segments with that GNIS) ->\n",
    "    unmatched_wdids = set(pods_t1[\"WDID\"]) - set(matched[\"WDID\"]) if not pods_t1.empty else set()\n",
    "    if unmatched_wdids:\n",
    "        pods_unmatched = pods.loc[pods[\"WDID\"].isin(unmatched_wdids), [\"WDID\", \"geometry\"]].copy()\n",
    "        # Vectorized nearest-anywhere using spatial index\n",
    "        nearest_any = gpd.sjoin_nearest(\n",
    "            pods_unmatched,\n",
    "            flow[[\"comid\", \"gnis_id\", \"gnis_name\", \"geometry\"]],\n",
    "            how=\"left\",\n",
    "            distance_col=\"dist\"\n",
    "        )\n",
    "        for r in nearest_any.itertuples(index=False):\n",
    "            res_comid[r.WDID] = int(r.comid) if pd.notna(r.comid) else None\n",
    "            res_gnis_name[r.WDID] = r.gnis_name\n",
    "            res_gnis_id[r.WDID] = r.gnis_id\n",
    "\n",
    "remaining_wdids = set(pods[\"WDID\"]) - set(res_comid.keys())\n",
    "pods_remain = pods[pods[\"WDID\"].isin(remaining_wdids)].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Tier 2: normalized name join\n",
    "# Condition WATER_SOURCE != 'tbd'\n",
    "# If no name matches -> fallback to streamorde>=3 (Tier 3), and increment fallback3_count\n",
    "# -------------------------\n",
    "t2_mask = pods_remain[\"WATER_SOURCE_str\"].str.lower().ne(\"tbd\")\n",
    "pods_t2 = pods_remain.loc[t2_mask, [\"WDID\", \"geometry\", \"ws_norm\"]].rename(columns={\"geometry\": \"geom_pod\"}).copy()\n",
    "\n",
    "pods_t2 = pods_t2[pods_t2[\"ws_norm\"].astype(str).str.len() > 0].copy()\n",
    "\n",
    "if not pods_t2.empty:\n",
    "    cand_t2 = pods_t2.merge(\n",
    "        flow[[\"comid\", \"gnis_id\", \"gnis_name\", \"gnis_name_norm\", \"geometry\"]].rename(columns={\"geometry\": \"geom_seg\"}),\n",
    "        left_on=\"ws_norm\",\n",
    "        right_on=\"gnis_name_norm\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    matched2 = cand_t2[cand_t2[\"comid\"].notna()].copy()\n",
    "    if not matched2.empty:\n",
    "        matched2[\"dist\"] = matched2[\"geom_seg\"].distance(matched2[\"geom_pod\"])\n",
    "        best2_idx = matched2.groupby(\"WDID\")[\"dist\"].idxmin()\n",
    "        best2 = matched2.loc[best2_idx]\n",
    "\n",
    "        for r in best2.itertuples(index=False):\n",
    "            res_comid[r.WDID] = int(r.comid) if pd.notna(r.comid) else None\n",
    "            res_gnis_name[r.WDID] = r.gnis_name\n",
    "            res_gnis_id[r.WDID] = r.gnis_id\n",
    "\n",
    "    matched2_wdids = set(matched2[\"WDID\"]) if not matched2.empty else set()\n",
    "    t2_no_match_wdids = set(pods_t2[\"WDID\"]) - matched2_wdids\n",
    "else:\n",
    "    t2_no_match_wdids = set()\n",
    "\n",
    "# Remaining after Tier 2 (go to Tier 3 iteration)\n",
    "remaining_wdids = set(pods[\"WDID\"]) - set(res_comid.keys())\n",
    "pods_t3 = pods[pods[\"WDID\"].isin(remaining_wdids)].copy()\n",
    "\n",
    "# Build streamorde>=3 subset\n",
    "flow_stream3 = flow[flow[\"streamorde\"].fillna(0) >= 3].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Tier 3: ITERATION \n",
    "# This tier applies to:\n",
    "#   - WATER_SOURCE == 'tbd'\n",
    "#   - OR Tier 2 produced no name match\n",
    "# -------------------------\n",
    "t2_no_match_wdids = set(t2_no_match_wdids)\n",
    "pods_t3_idx = list(pods_t3.index)\n",
    "\n",
    "for idx in tqdm(pods_t3_idx, total=len(pods_t3_idx), desc=\"Tier 3 (streamorde>=3) POD linking\"):\n",
    "    row = pods.loc[idx]\n",
    "    wdid = row[\"WDID\"]\n",
    "    geom = row.geometry\n",
    "\n",
    "    # Count fallback3 like your function:\n",
    "    # - if WATER_SOURCE == tbd  -> fallback3\n",
    "    # - if WATER_SOURCE != tbd but no match in Tier2 -> fallback3\n",
    "    ws_is_tbd = str(row.get(\"WATER_SOURCE\", \"\")).strip().lower() == \"tbd\"\n",
    "    if ws_is_tbd or (wdid in t2_no_match_wdids):\n",
    "        fallback3_count += 1\n",
    "        fallback3_ids.append(wdid)\n",
    "\n",
    "    # If there are no streamorde>=3 segments\n",
    "    if flow_stream3.empty:\n",
    "        fallback4_count += 1\n",
    "        no_link_count += 1\n",
    "        no_link_ids.append(wdid)\n",
    "        no_link_records.append({\n",
    "            \"WDID\": wdid,\n",
    "            \"geometry\": geom,\n",
    "            \"SOURCE_GNIS_ID\": row.get(\"SOURCE_GNIS_ID\", None),\n",
    "            \"Source GNIS ID Raw\": row.get(\"Source GNIS ID Raw\", None),\n",
    "            \"WATER_SOURCE\": row.get(\"WATER_SOURCE\", None),\n",
    "            \"reason\": \"no_streamorde>=3_segment\"\n",
    "        })\n",
    "        res_comid[wdid] = None\n",
    "        res_gnis_name[wdid] = None\n",
    "        res_gnis_id[wdid] = None\n",
    "        continue\n",
    "\n",
    "    # Nearest among streamorde>=3\n",
    "    dists = flow_stream3.geometry.distance(geom)\n",
    "    best_i = dists.idxmin()\n",
    "    seg = flow_stream3.loc[best_i]\n",
    "\n",
    "    res_comid[wdid] = seg[feature_name]\n",
    "    res_gnis_name[wdid] = seg[\"gnis_name\"]\n",
    "    res_gnis_id[wdid] = seg[\"gnis_id\"]\n",
    "\n",
    "pods[\"SOURCE_COMID\"] = pods[\"WDID\"].map(res_comid)\n",
    "pods[\"WATER_SOURCE\"] = pods[\"WDID\"].map(res_gnis_name)\n",
    "pods[\"SOURCE_GNIS_ID\"] = pods[\"WDID\"].map(res_gnis_id)\n",
    "pods = pods.drop(columns=[\"Source GNIS ID Raw\", \"SOURCE_GNIS_ID_num\", \"WATER_SOURCE_str\", \"ws_norm\"], errors=\"ignore\")\n",
    "POD_data = pods\n",
    "closest_features_res = []\n",
    "\n",
    "for idx, point in tqdm(res_data.iterrows(),total=len(res_data), desc=\"Linking Reservoir IDs\") :\n",
    "    river_name = res_data.at[idx, 'RIVER']\n",
    "    point_geom = point.geometry\n",
    "    closest_segment_test = closest_segment_func2(point, flow_layer)\n",
    "    closest_segment = walk_downstream_past_lakepond(closest_segment_test, flow_layer)\n",
    "    \n",
    "    closest_feature_value = closest_segment[feature_name] if closest_segment is not None else None\n",
    "    closest_features_res.append(closest_feature_value)\n",
    "\n",
    "res_data['SOURCE_COMID'] = closest_features_res\n",
    "res_data['POI_NATIVE_ID'] = res_data['NID_ID']\n",
    "\n",
    "#gage_data['SOURCE_COMID'] = np.nan\n",
    "#gage_data.loc[gage_data['hl_reference'] == 'type_gages', 'Source_' + feature_name] = gage_data['hy_id']\n",
    "\n",
    "#gage_data['POI_NativeID'] = np.nan\n",
    "#gage_data.loc[gage_data['hl_reference'] == 'type_gages', 'POI_NativeID'] = gage_data['hl_link']\n",
    "\n",
    "print(f\"\\nNumber of PODs that used fallback 3: {fallback3_count}\")\n",
    "print(f\"\\nNumber of PODs that used fallback 4: {fallback4_count}\")\n",
    "print(f\"\\nNumber of Reservoirs that used fallback 3: {hf.res_fallback_counter}\")\n",
    "\n",
    "enhanced_fabric_path = f'data/output/enhanced_reference_{huc_code}.gpkg'\n",
    "shutil.copyfile(reference_fabric_path, enhanced_fabric_path)\n",
    "\n",
    "# with fiona.Env():\n",
    "#     fiona.remove(enhanced_fabric_path, layer='event')\n",
    "\n",
    "#gage_data.to_file(enhanced_fabric_path, layer='event', driver='GPKG', mode='a')\n",
    "POD_data.to_file(enhanced_fabric_path, layer='DIVERSION_POINTS', driver='GPKG', mode='a')\n",
    "res_data.to_file(enhanced_fabric_path, layer='RESERVOIR_POINTS', driver='GPKG', mode='a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e852b964afdf72",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
