{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from dataretrieval import nwis\n",
    "import geopandas as gpd\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from tqdm import tqdm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "huc_code = '14'\n",
    "huc_feature = 'huc2'\n",
    "\n",
    "\n",
    "ROW_CHUNK = 1000   \n",
    "VAR_ID = 5       \n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdca0234559702a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# Function to insert data into POI_TYPE table\n",
    "def insert_poi_type(poi_type_name, poi_type_source):\n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            SELECT POI_TYPE_ID FROM POI_TYPE WHERE POI_TYPE_NAME = ? AND POI_TYPE_SOURCE = ?\n",
    "        ''', (poi_type_name, poi_type_source))\n",
    "        if cursor.fetchone() is None:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO POI_TYPE (POI_TYPE_NAME, POI_TYPE_SOURCE)\n",
    "                VALUES (?, ?)\n",
    "            ''', (poi_type_name, poi_type_source))\n",
    "            print(f\"Inserted {poi_type_name} into POI_TYPE\")\n",
    "        else:\n",
    "            print(f\"{poi_type_name} already exists in POI_TYPE\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Function to insert data into VARIABLES table\n",
    "def insert_variable(variable_name, unit):\n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            SELECT VARIABLE_ID FROM VARIABLES WHERE VARIABLE_NAME = ? AND UNIT = ?\n",
    "        ''', (variable_name, unit))\n",
    "        if cursor.fetchone() is None:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO VARIABLES (VARIABLE_NAME, UNIT)\n",
    "                VALUES (?, ?)\n",
    "            ''', (variable_name, unit))\n",
    "            print(f\"Inserted {variable_name} into VARIABLES\")\n",
    "        else:\n",
    "            print(f\"{variable_name} already exists in VARIABLES\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "# ---------- FAST LOOKUP + BULK INSERT HELPERS ----------\n",
    "def standardize_datetime(dt):\n",
    "    \"\"\"\n",
    "    Return a standardized ISO-8601 string for a date/datetime-like input.\n",
    "    - If only a date is present -> 'YYYY-MM-DD'\n",
    "    - If time is present       -> 'YYYY-MM-DDTHH:MM:SS'\n",
    "    \"\"\"\n",
    "    if dt is None or (isinstance(dt, float) and pd.isna(dt)):\n",
    "        return None\n",
    "\n",
    "    # Fast path for already-clean pandas Timestamps/DatetimeIndex\n",
    "    try:\n",
    "        ts = pd.to_datetime(dt, errors='coerce', infer_datetime_format=True, utc=False)\n",
    "    except Exception:\n",
    "        ts = pd.NaT\n",
    "\n",
    "    if pd.isna(ts):\n",
    "        return None\n",
    "\n",
    "    # If tz-aware, drop tz (store as local naive)\n",
    "    try:\n",
    "        if getattr(ts, 'tzinfo', None) is not None:\n",
    "            ts = ts.tz_convert(None)\n",
    "    except Exception:\n",
    "        # Some types use tz_localize(None)\n",
    "        try:\n",
    "            ts = ts.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Decide whether to emit date-only or date-time\n",
    "    src = str(dt)\n",
    "    has_time_in_src = ('T' in src) or (':' in src) or (len(src.strip()) > 10)\n",
    "\n",
    "    if not has_time_in_src and ts.hour == 0 and ts.minute == 0 and ts.second == 0:\n",
    "        return ts.date().isoformat()  # 'YYYY-MM-DD'\n",
    "    else:\n",
    "        return ts.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def rows_for_site_id(wdid: str):\n",
    "    \"\"\"\n",
    "    Return a DataFrame (possibly empty) of WaDE rights for a given SiteNativeId.\n",
    "    Uses the index for O(1)/hashed access and avoids per-iteration astype/scan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hits = wade_df.loc[wdid]  # could be Series (single) or DataFrame (multiple)\n",
    "    except KeyError:\n",
    "        return pd.DataFrame(columns=wade_df.columns)\n",
    "\n",
    "    if isinstance(hits, pd.Series):\n",
    "        # single match -> make it a one-row DataFrame\n",
    "        return hits.to_frame().T\n",
    "    return hits\n",
    "\n",
    "def bulk_insert_pod_waterrights(rows, chunk=50000):\n",
    "    \"\"\"\n",
    "    rows: iterable of tuples (POIID, SiteName, WaterRightID, Allocation_CFS,\n",
    "                              Allocation_Date, Use_Type, Water_Source, Source_ID)\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return\n",
    "    sql = '''\n",
    "        INSERT INTO POD_WATER_RIGHTS\n",
    "            (POI_ID, SITE_NAME, WATER_RIGHT_ID, ALLOCATION_CFS, ALLOCATION_DATE,\n",
    "             USE_TYPE, WATER_SOURCE, SOURCE_ID)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    '''\n",
    "    # Standardize the Allocation_Date column before insert\n",
    "    rows_std = []\n",
    "    for (pid, sname, wid, cfs, adate, utype, wsrc, srcid) in rows:\n",
    "        rows_std.append((pid, sname, wid, cfs, standardize_datetime(adate), utype, wsrc, srcid))\n",
    "\n",
    "    n = len(rows_std)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + chunk, n)\n",
    "        cursor.executemany(sql, rows_std[start:end])\n",
    "        start = end\n",
    "\n",
    "def bulk_insert_poi_values(rows, chunk=2000000):\n",
    "    \"\"\"\n",
    "    rows: iterable of (DataValue, LocalDateTime, POIID, VariableID)\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return\n",
    "    sql = '''\n",
    "        INSERT INTO POI_VALUES (DATA_VALUE, LOCAL_DATE_TIME, POI_ID, VARIABLE_ID)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    '''\n",
    "    # Map timestamps on the way in\n",
    "    rows_std = [\n",
    "        (dv, standardize_datetime(ldt), pid, vid)\n",
    "        for (dv, ldt, pid, vid) in rows\n",
    "    ]\n",
    "    start = 0\n",
    "    n = len(rows_std)\n",
    "    while start < n:\n",
    "        end = min(start + chunk, n)\n",
    "        cursor.executemany(sql, rows_std[start:end])\n",
    "        start = end\n",
    "\n",
    "def load_all_wade(root_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find */Sites.csv and */WaterAllocations.csv under root_dir, normalize column names,\n",
    "    merge on SiteUuid, explode multi-site allocations, and return a single DataFrame\n",
    "    indexed by SiteNativeId (string).\n",
    "    \"\"\"\n",
    "    import os, glob\n",
    "    import pandas as pd\n",
    "\n",
    "    site_files = glob.glob(os.path.join(root_dir, \"**\", \"Sites.csv\"), recursive=True)\n",
    "    parts = []\n",
    "\n",
    "    def pick(colmap, candidates):\n",
    "        \"\"\"Finds first matching column in candidates (case-insensitive).\"\"\"\n",
    "        clower = {c.lower(): c for c in candidates}\n",
    "        for name in colmap:\n",
    "            if name.lower() in clower:\n",
    "                return clower[name.lower()]\n",
    "        return None\n",
    "\n",
    "    for sites_path in site_files:\n",
    "        alloc_path = os.path.join(os.path.dirname(sites_path), \"WaterAllocations.csv\")\n",
    "        if not os.path.exists(alloc_path):\n",
    "            continue\n",
    "\n",
    "        sites_df = pd.read_csv(sites_path)\n",
    "        alloc_df = pd.read_csv(alloc_path)\n",
    "\n",
    "        # ----- resolve keys -----\n",
    "        sites_siteuuid = pick([\"SiteUuid\", \"SiteUUID\"], sites_df.columns)\n",
    "        alloc_siteuuid = pick([\"SiteUuid\", \"SiteUUID\"], alloc_df.columns)\n",
    "\n",
    "        if not sites_siteuuid or not alloc_siteuuid:\n",
    "            continue\n",
    "\n",
    "        # ----- pick important columns -----\n",
    "        sites_sitenativeid = pick([\"SiteNativeId\", \"SiteNativeID\"], sites_df.columns)\n",
    "        sites_sitename     = pick([\"SiteName\"], sites_df.columns)\n",
    "        sites_lat          = pick([\"Latitude\", \"Latitude_DD\"], sites_df.columns)\n",
    "        sites_lon          = pick([\"Longitude\", \"Longitude_DD\"], sites_df.columns)\n",
    "\n",
    "        alloc_nativeid     = pick([\"AllocationNativeID\", \"AllocationNativeId\"], alloc_df.columns)\n",
    "        alloc_flow_cfs     = pick([\"AllocationFlow_CFS\", \"AllocationFlow_Cfs\"], alloc_df.columns)\n",
    "        alloc_priority     = pick([\"AllocationPriorityDate\", \"PriorityDate\"], alloc_df.columns)\n",
    "        alloc_beneficial   = pick([\"BeneficialUseCategory\", \"BeneficialUseCategoryCV\"], alloc_df.columns)\n",
    "\n",
    "        # ----- explode multi-site UUIDs -----\n",
    "        # If SiteUUID field contains comma-separated values\n",
    "        if alloc_siteuuid in alloc_df.columns:\n",
    "            alloc_df[alloc_siteuuid] = alloc_df[alloc_siteuuid].astype(str).str.split(\",\")\n",
    "            alloc_df = (\n",
    "                alloc_df\n",
    "                .explode(alloc_siteuuid)\n",
    "                .assign(**{alloc_siteuuid: lambda d: d[alloc_siteuuid].str.strip()})\n",
    "            )\n",
    "\n",
    "        # ----- normalize and rename -----\n",
    "        sites_keep = {\n",
    "            sites_siteuuid: \"SiteUuid\",\n",
    "            sites_sitenativeid: \"SiteNativeId\",\n",
    "            sites_sitename if sites_sitename else sites_siteuuid: \"SiteName\",\n",
    "            sites_lat: \"Latitude\",\n",
    "            sites_lon: \"Longitude\",\n",
    "        }\n",
    "        sites_norm = sites_df[list(sites_keep.keys())].rename(columns=sites_keep)\n",
    "\n",
    "        alloc_keep = {\n",
    "            alloc_siteuuid: \"SiteUuid\",\n",
    "            alloc_nativeid: \"AllocationNativeID\",\n",
    "            alloc_priority: \"AllocationPriorityDate\",\n",
    "        }\n",
    "        if alloc_flow_cfs:\n",
    "            alloc_keep[alloc_flow_cfs] = \"AllocationFlow_CFS\"\n",
    "        else:\n",
    "            alloc_df[\"__AllocationFlow_CFS__\"] = pd.NA\n",
    "            alloc_keep[\"__AllocationFlow_CFS__\"] = \"AllocationFlow_CFS\"\n",
    "        if alloc_beneficial:\n",
    "            alloc_keep[alloc_beneficial] = \"BeneficialUseCategory\"\n",
    "        else:\n",
    "            alloc_df[\"__BeneficialUseCategory__\"] = pd.NA\n",
    "            alloc_keep[\"__BeneficialUseCategory__\"] = \"BeneficialUseCategory\"\n",
    "\n",
    "        alloc_norm = alloc_df[list(alloc_keep.keys())].rename(columns=alloc_keep)\n",
    "\n",
    "        # ----- merge -----\n",
    "        merged = alloc_norm.merge(sites_norm, on=\"SiteUuid\", how=\"inner\")\n",
    "        parts.append(merged)\n",
    "\n",
    "    if not parts:\n",
    "        cols = [\n",
    "            \"SiteUuid\", \"SiteNativeId\", \"SiteName\",\n",
    "            \"AllocationNativeID\", \"AllocationFlow_CFS\",\n",
    "            \"AllocationPriorityDate\", \"BeneficialUseCategory\",\n",
    "            \"Latitude\", \"Longitude\",\n",
    "        ]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    wade_all = pd.concat(parts, ignore_index=True)\n",
    "    wade_all[\"SiteNativeId\"] = wade_all[\"SiteNativeId\"].astype(str)\n",
    "    wade_all.set_index(\"SiteNativeId\", inplace=True, drop=False)\n",
    "    return wade_all\n",
    "\n",
    "\n",
    "def insert_poi(poiid, poi_type_id, poi_lat, poi_lon, poi_native_id, poi_flow_com_id):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO POI (POI_ID, POI_TYPE_ID, POI_LAT, POI_LON, POI_NATIVE_ID, POI_FLOW_COMID)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''',\n",
    "            (poiid, poi_type_id, poi_lat, poi_lon, poi_native_id, poi_flow_com_id),\n",
    "        )\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def insert_poi_values(dataval, localtime, poiid, variableid):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO POI_VALUES (DATA_VALUE, LOCAL_DATE_TIME, POI_ID, VARIABLE_ID)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ''',\n",
    "            (dataval, standardize_datetime(localtime), poiid, variableid),\n",
    "        )\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def poiid_exists(poiid):\n",
    "    cursor.execute('SELECT 1 FROM POI WHERE POI_ID = ?', (poiid,))\n",
    "    return cursor.fetchone() is not None\n",
    "\n",
    "def get_site_coordinates(site_number):\n",
    "    try:\n",
    "        site_info = nwis.get_info(sites=site_number)\n",
    "        latitude = site_info[0]['dec_lat_va'].iloc[0]\n",
    "        longitude = site_info[0]['dec_long_va'].iloc[0]\n",
    "        return latitude, longitude\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving coordinates for site {site_number}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def insert_pod_waterrights(poiid, site_name, right_id, allocation_cfs, allocation_date,\n",
    "                           use_type, water_source, source_id):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO POD_WATER_RIGHTS\n",
    "                (POI_ID, SITE_NAME, WATER_RIGHT_ID, ALLOCATION_CFS, ALLOCATION_DATE,\n",
    "                 USE_TYPE, WATER_SOURCE, SOURCE_ID)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''',\n",
    "            (poiid, site_name, right_id, allocation_cfs, standardize_datetime(allocation_date),\n",
    "             use_type, water_source, source_id),\n",
    "        )\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "def yield_payload():\n",
    "    import numpy as np\n",
    "    for start in range(0, len(demands_df), ROW_CHUNK):\n",
    "        end = min(start + ROW_CHUNK, len(demands_df))\n",
    "        sub_vals = demands_df.iloc[start:end][valid_wdid_cols]\n",
    "        sub_date = date_std.iloc[start:end]\n",
    "\n",
    "        # wide -> long just for this slice\n",
    "        sub_long = (\n",
    "            sub_vals.assign(__Date=sub_date.values)\n",
    "                    .melt(id_vars='__Date', var_name='WDID', value_name='DataValue')\n",
    "                    .dropna(subset=['DataValue'])\n",
    "        )\n",
    "        if sub_long.empty:\n",
    "            continue\n",
    "\n",
    "        # map to POI ids\n",
    "        sub_long['POI_ID'] = sub_long['WDID'].map(wdid_to_poi)\n",
    "\n",
    "        # build arrays, not Python lists (smaller overhead), then zip\n",
    "        dv = sub_long['DataValue'].to_numpy()\n",
    "        ldt = sub_long['__Date'].to_numpy()      # already standardized strings\n",
    "        pid = sub_long['POI_ID'].to_numpy()\n",
    "\n",
    "        # yield tuples to our bulk insert with \"already standardized time\"\n",
    "        for t in zip(dv, ldt, pid, repeat(5, len(dv))):\n",
    "            yield t\n",
    "\n",
    "def bulk_insert_poi_values_fast(rows_iter, chunk=200000):\n",
    "    sql = '''\n",
    "        INSERT INTO POI_VALUES (DATA_VALUE, LOCAL_DATE_TIME, POI_ID, VARIABLE_ID)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    '''\n",
    "    batch = []\n",
    "    for r in rows_iter:\n",
    "        batch.append(r)\n",
    "        if len(batch) >= chunk:\n",
    "            cursor.executemany(sql, batch)\n",
    "            batch.clear()\n",
    "    if batch:\n",
    "        cursor.executemany(sql, batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd23b143b0f0673",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Directories\n",
    "\n",
    "db_path= f'data/output/optional_db_{huc_code}.db'\n",
    "\n",
    "# extended hydrofabric geopackage (can be HUC4-specific)\n",
    "extended_hydrofabric = f'data/output/enhanced_reference_14.gpkg'\n",
    "\n",
    "# ResOpsUS reservoir attributes + folder with timeseries\n",
    "csv_file_path = 'data/reservoirs/updated_reservoir_attributes.csv'\n",
    "res_folder_path = 'data/reservoirs/time_series_all/'\n",
    "\n",
    "# Demand time series for diversions\n",
    "demands_df = pd.read_csv('data/USGS (Lopez) Demands/diversion_records_wadeID.csv')\n",
    "demands_df.columns = demands_df.columns.map(str)   # <-- force all column names to str\n",
    "\n",
    "agg_csv = 'data/*' # Optional, If you have aggregated diversions place them in the folder with the correct structure (see example)\n",
    "if os.path.exists(agg_csv):\n",
    "    aggregated_diversions_df = pd.read_csv(agg_csv)\n",
    "    aggregated_diversions_df['Aggregation ID'] = aggregated_diversions_df['Aggregation ID'].astype(str).str.strip()\n",
    "else:\n",
    "    print(\" aggregated_table.csv not found. Continuing without aggregated diversion data.\")\n",
    "\n",
    "# Root folder containing per-state WaDE folders; each folder has Sites.csv + WaterAllocations.csv\n",
    "# Example layout:\n",
    "# data/WaDE_all_states/\n",
    "#   Colorado_WaDE/Sites.csv\n",
    "#   Colorado_WaDE/WaterAllocations.csv\n",
    "#   Utah_WaDE/Sites.csv\n",
    "#   Utah_WaDE/WaterAllocations.csv\n",
    "WADE_ROOT = \"data/wade data\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "pod_layer = gpd.read_file(extended_hydrofabric, layer='DIVERSION_POINTS')\n",
    "gage_layer = gpd.read_file(extended_hydrofabric, layer='event')\n",
    "res_layer  = gpd.read_file(extended_hydrofabric, layer='RESERVOIR_POINTS')\n",
    "\n",
    "# Boundary clipping for gages\n",
    "boundary = gpd.read_file('data/wbd/WBDHU2.shp')\n",
    "huc_wbd = boundary[boundary[huc_feature] == huc_code].to_crs(epsg=4326)\n",
    "\n",
    "gage_layer = gage_layer.to_crs(huc_wbd.crs)\n",
    "events_selected = gage_layer[gage_layer.geometry.within(huc_wbd.unary_union)]\n",
    "gage_filtered = events_selected.loc[events_selected['hl_reference'] == 'type_gages']\n",
    "USGS_gage_ids = gage_filtered['hl_link'].tolist()\n",
    "\n",
    "# IDs for reservoirs & diversions\n",
    "dam_name_ids = res_layer['NID_ID'].tolist()\n",
    "\n",
    "# Filter DataFrame for existing dams in attributes table\n",
    "filtered_df = df[df['NID_ID'].isin(dam_name_ids)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55f36c5ce6761b1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Blank database schema creation\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table for POI_TYPE\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS POI_TYPE (\n",
    "    POI_TYPE_ID INTEGER PRIMARY KEY,\n",
    "    POI_TYPE_NAME TEXT,\n",
    "    POI_TYPE_SOURCE TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for POI\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS POI (\n",
    "    POI_ID TEXT PRIMARY KEY,\n",
    "    POI_TYPE_ID INTEGER,\n",
    "    POI_LAT REAL,\n",
    "    POI_LON REAL,\n",
    "    POI_NATIVE_ID TEXT,\n",
    "    POI_FLOW_COMID INTEGER,\n",
    "    FOREIGN KEY (POI_TYPE_ID) REFERENCES POI_TYPE (POI_TYPE_ID)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for VARIABLES\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS VARIABLES (\n",
    "    VARIABLE_ID INTEGER PRIMARY KEY,\n",
    "    VARIABLE_NAME TEXT,\n",
    "    UNIT TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for POI_VALUES\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS POI_VALUES (\n",
    "    VALUE_ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATA_VALUE REAL,\n",
    "    LOCAL_DATE_TIME TEXT,\n",
    "    POI_ID TEXT,\n",
    "    VARIABLE_ID INTEGER,\n",
    "    FOREIGN KEY (POI_ID) REFERENCES POI (POI_ID),\n",
    "    FOREIGN KEY (VARIABLE_ID) REFERENCES VARIABLES (VARIABLE_ID)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for ET_PRECIP\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS ET_PRECIP (\n",
    "    ET_PRECIP_ID INTEGER PRIMARY KEY,\n",
    "    ET_DA_VALUE REAL,\n",
    "    PRECIP_VALUE REAL,\n",
    "    LOCAL_DATE_TIME TEXT,\n",
    "    POI_ID TEXT,\n",
    "    VARIABLE_ID INTEGER,\n",
    "    FOREIGN KEY (POI_ID) REFERENCES POI (POI_ID),\n",
    "    FOREIGN KEY (VARIABLE_ID) REFERENCES VARIABLES (VARIABLE_ID)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for RULE_CURVES\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS RULE_CURVES (\n",
    "    RULE_CURVE_ID INTEGER PRIMARY KEY,\n",
    "    MIN_RELEASE_VALUE REAL,\n",
    "    TARGET_STORAGE_VALUE REAL,\n",
    "    LOCAL_DATE_TIME TEXT,\n",
    "    POI_ID TEXT,\n",
    "    VARIABLE_ID INTEGER,\n",
    "    FOREIGN KEY (POI_ID) REFERENCES POI (POI_ID),\n",
    "    FOREIGN KEY (VARIABLE_ID) REFERENCES VARIABLES (VARIABLE_ID)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create table for POD_WATER_RIGHTS\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS POD_WATER_RIGHTS (\n",
    "    POD_WATER_RIGHTS_ID INTEGER PRIMARY KEY,\n",
    "    POI_ID TEXT,\n",
    "    SITE_NAME TEXT,\n",
    "    WATER_RIGHT_ID TEXT,\n",
    "    ALLOCATION_CFS REAL,\n",
    "    ALLOCATION_DATE TEXT,\n",
    "    USE_TYPE TEXT,\n",
    "    WATER_SOURCE TEXT,\n",
    "    SOURCE_ID REAL,\n",
    "    FOREIGN KEY (POI_ID) REFERENCES POI (POI_ID)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eab85ac950748917",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# fixed variable inserts\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Data inserted into POI_TYPE\n",
    "poi_types = [\n",
    "    ('USGS_GAGE', 'USGS'),\n",
    "    # ('STATE_GAGE', 'CDSS'),\n",
    "    ('POD', 'WaDE'),\n",
    "    ('RESERVOIR', 'ResOpsUS')\n",
    "]\n",
    "\n",
    "for poi_type in poi_types:\n",
    "    insert_poi_type(poi_type[0], poi_type[1])\n",
    "\n",
    "# Data inserted into VARIABLES\n",
    "variables = [\n",
    "    ('INFLOW', 'CMS'),\n",
    "    ('OUTFLOW', 'CMS'),\n",
    "    ('STORAGE', 'MCM'),\n",
    "    ('GAGE_FLOW', 'CFS'),\n",
    "    ('DEMAND', 'CM')\n",
    "]\n",
    "\n",
    "for variable in variables:\n",
    "    insert_variable(variable[0], variable[1])\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c23e51fec8f3c930",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# LOAD *ALL STATES* WADE (STATE-AGNOSTIC)\n",
    "# --------------------------\n",
    "wade_df = load_all_wade(WADE_ROOT)  # wade_df is indexed by SiteNativeId (string)\n",
    "\n",
    "\n",
    "print(\"Wade loaded\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d5a00e50f0703fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PROCESS RESERVOIRS (ResOpsUS)\n",
    "\n",
    "# SQLite DB target\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"PRAGMA journal_mode = WAL;\")      \n",
    "cursor.execute(\"PRAGMA synchronous = NORMAL;\")  \n",
    "cursor.execute(\"PRAGMA temp_store = MEMORY;\")     \n",
    "cursor.execute(\"PRAGMA cache_size = -200000;\") \n",
    "\n",
    "for _, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"Processing Reservoir Data\"):\n",
    "    poiid = f\"{row['DAM_ID']}_{row['NID_ID']}\"\n",
    "\n",
    "    if poiid_exists(poiid):\n",
    "        continue\n",
    "\n",
    "    # Find the Source_comid for this reservoir\n",
    "    comid_vals = res_layer.loc[res_layer['NID_ID'] == row['NID_ID'], 'SOURCE_COMID'].values\n",
    "    poi_flow_com_id = comid_vals[0] if len(comid_vals) > 0 else None\n",
    "\n",
    "    insert_poi(\n",
    "        poiid=poiid,\n",
    "        poi_type_id=3,\n",
    "        poi_lat=row['LATITUDE'],\n",
    "        poi_lon=row['LONGITUDE'],\n",
    "        poi_native_id=row['NID_ID'],\n",
    "        poi_flow_com_id=poi_flow_com_id\n",
    "    )\n",
    "\n",
    "    file_name = f\"ResOpsUS_{row['DAM_ID']}.csv\"\n",
    "    file_path = os.path.join(res_folder_path, file_name)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        dam_df = pd.read_csv(file_path)\n",
    "        variable_names = ['inflow', 'outflow', 'storage']\n",
    "        var_id_start = 1\n",
    "\n",
    "        for var_id, var_name in enumerate(variable_names, start=var_id_start):\n",
    "            if var_name in dam_df.columns:\n",
    "                for _, dam_row in dam_df.iterrows():\n",
    "                    insert_poi_values(\n",
    "                        dataval=dam_row[var_name],\n",
    "                        localtime=dam_row['date'],\n",
    "                        poiid=poiid,\n",
    "                        variableid=var_id,\n",
    "                    )\n",
    "                    \n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "931fdf879fe9484b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PROCESS USGS GAGES\n",
    "# --------------------------\n",
    "\n",
    "# SQLite DB target\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"PRAGMA journal_mode = WAL;\")      \n",
    "cursor.execute(\"PRAGMA synchronous = NORMAL;\")    \n",
    "cursor.execute(\"PRAGMA temp_store = MEMORY;\")     \n",
    "cursor.execute(\"PRAGMA cache_size = -200000;\")   \n",
    "\n",
    "for gage_id in tqdm (USGS_gage_ids, total = len(USGS_gage_ids), desc= \"Processing USGS Dage Data\"):\n",
    "    poiid = f\"USGS_{gage_id}\"\n",
    "\n",
    "    if poiid_exists(poiid):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        nwis_data_raw = nwis.get_dv(sites=gage_id, parameterCd='00060', statCd='00003', startDT='1880-01-01')\n",
    "\n",
    "        nwis_data = nwis_data_raw[0]\n",
    "        latitude, longitude = get_site_coordinates(gage_id)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "    def pick_00060_mean_column(df):\n",
    "        # case-insensitive match for \"mean\" to handle \"Mean\" vs \"mean\"\n",
    "        candidates = [c for c in df.columns if c.startswith('00060') and 'mean' in c.lower()]\n",
    "        if not candidates:\n",
    "            raise KeyError(\"No 00060 mean column found.\")\n",
    "        # Prefer the unsuffixed version (e.g., '00060_Mean') if it exists\n",
    "        unsuffixed = [c for c in candidates if c.lower() in ('00060_mean', '00060_mean')]\n",
    "        if unsuffixed:\n",
    "            return unsuffixed[0]\n",
    "        # Otherwise just pick the first candidate (e.g., '00060_2_mean')\n",
    "        return sorted(candidates)[0]\n",
    "\n",
    "    comid_vals = gage_layer.loc[gage_layer['hl_link'] == gage_id, 'hy_id'].values\n",
    "    poi_flow_com_id = comid_vals[0] if len(comid_vals) > 0 else None\n",
    "\n",
    "    if not nwis_data.empty and latitude is not None and longitude is not None:\n",
    "        insert_poi(\n",
    "            poiid=poiid,\n",
    "            poi_type_id=1,\n",
    "            poi_lat=latitude,\n",
    "            poi_lon=longitude,\n",
    "            poi_native_id=gage_id,\n",
    "            poi_flow_com_id=poi_flow_com_id\n",
    "        )\n",
    "\n",
    "        nwis_data = nwis_data_raw[0]\n",
    "        flow_col = pick_00060_mean_column(nwis_data)\n",
    "\n",
    "        for index, entry in nwis_data.iterrows():\n",
    "            insert_poi_values(\n",
    "                dataval=entry[flow_col],\n",
    "                localtime=index.isoformat(),\n",
    "                poiid=poiid,\n",
    "                variableid=4,\n",
    "            )\n",
    "    else:\n",
    "        print(f\"No data found or incomplete data for gage ID: {gage_id}\")\n",
    "        \n",
    "        \n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7d6c8a240a4aaa2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PROCESS PODs and Water Rights\n",
    "# --------------------------\n",
    "\n",
    "# SQLite DB target\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"PRAGMA journal_mode = WAL;\")      \n",
    "cursor.execute(\"PRAGMA synchronous = NORMAL;\")    \n",
    "cursor.execute(\"PRAGMA temp_store = MEMORY;\")     \n",
    "cursor.execute(\"PRAGMA cache_size = -200000;\")  \n",
    "\n",
    "wdid_to_poi = {str(row['WDID']): f\"DIV_{str(row['WDID'])}\" for _, row in pod_layer.iterrows()}\n",
    "\n",
    "for _, diversion_row in tqdm(pod_layer.iterrows(), total = len(pod_layer), desc = \"Processing Diversion Points and Water Rights\"):\n",
    "    wdid = str(diversion_row['WDID'])\n",
    "    poi_id = f\"DIV_{wdid}\"\n",
    "    latitude = diversion_row['LATITUDE']\n",
    "    longitude = diversion_row['LONGITUDE']\n",
    "    flowline_comid = diversion_row['SOURCE_COMID']\n",
    "    diversion_type = str(diversion_row['TYPE']).strip()\n",
    "    water_source = diversion_row['WATER_SOURCE']\n",
    "    source_gnis_id = diversion_row['SOURCE_GNIS_ID']\n",
    "    native_id = diversion_row['POI_NATIVE_ID']\n",
    "\n",
    "    # Create POI (idempotent is fine; rely on unique PK to avoid dupes if needed)\n",
    "    insert_poi(\n",
    "        poiid=poi_id,\n",
    "        poi_type_id=2,\n",
    "        poi_lat=latitude,\n",
    "        poi_lon=longitude,\n",
    "        poi_native_id=native_id,\n",
    "        poi_flow_com_id=flowline_comid\n",
    "    )\n",
    "\n",
    "\n",
    "    payload = []  # collect rights rows for this POI, then insert in bulk\n",
    "\n",
    "    if diversion_type == 'Physical':\n",
    "        matches = rows_for_site_id(wdid)\n",
    "        if not matches.empty:\n",
    "            for _, r in matches.iterrows():\n",
    "                payload.append((\n",
    "                    poi_id,\n",
    "                    r['SiteName'],\n",
    "                    r['AllocationNativeID'],\n",
    "                    r['AllocationFlow_CFS'],\n",
    "                    r['AllocationPriorityDate'],\n",
    "                    r['BeneficialUseCategory'],\n",
    "                    water_source,\n",
    "                    source_gnis_id\n",
    "                ))\n",
    "\n",
    "    elif os.path.exists(agg_csv) and diversion_type == 'Aggregated Diversion':\n",
    "        aggregation_matches = aggregated_df[\n",
    "            aggregated_df['Aggregation ID'].astype(str) == wdid\n",
    "        ]\n",
    "        for _, a in aggregation_matches.iterrows():\n",
    "            agg_wdid = str(a['WDID'])\n",
    "            matches = rows_for_site_id(agg_wdid)\n",
    "            if matches.empty:\n",
    "                continue\n",
    "            for _, r in matches.iterrows():\n",
    "                payload.append((\n",
    "                    poi_id,\n",
    "                    r['SiteName'],\n",
    "                    r['AllocationNativeID'],\n",
    "                    r['AllocationFlow_CFS'],\n",
    "                    r['AllocationPriorityDate'],\n",
    "                    r['BeneficialUseCategory'],\n",
    "                    water_source,\n",
    "                    source_gnis_id\n",
    "                ))\n",
    "    else:\n",
    "        # Fallback direct match\n",
    "        matches = rows_for_site_id(wdid)\n",
    "        if not matches.empty:\n",
    "            for _, r in matches.iterrows():\n",
    "                payload.append((\n",
    "                    poi_id,\n",
    "                    r['SiteName'],\n",
    "                    r['AllocationNativeID'],\n",
    "                    r['AllocationFlow_CFS'],\n",
    "                    r['AllocationPriorityDate'],\n",
    "                    r['BeneficialUseCategory'],\n",
    "                    water_source,\n",
    "                    source_gnis_id\n",
    "                ))\n",
    "\n",
    "    # single fast insert for this POD\n",
    "    bulk_insert_pod_waterrights(payload)\n",
    "    \n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4fbc22013776b3d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Processing Demands\n",
    "# --------------------------\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA foreign_keys = OFF;\")\n",
    "cursor.execute(\"PRAGMA synchronous = OFF;\")\n",
    "cursor.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
    "cursor.execute(\"PRAGMA temp_store = MEMORY;\")\n",
    "cursor.execute(\"PRAGMA locking_mode = EXCLUSIVE;\")\n",
    "cursor.execute(\"BEGIN IMMEDIATE;\")  \n",
    "\n",
    "valid_wdid_cols = [c for c in demands_df.columns if c != 'Date' and c in wdid_to_poi]\n",
    "if valid_wdid_cols:\n",
    "\n",
    "    dt = pd.to_datetime(demands_df['Date'], errors='coerce', utc=False)\n",
    "    date_std = (\n",
    "        pd.Series(\n",
    "            dt.dt.strftime('%Y-%m-%dT%H:%M:%S').where(\n",
    "                # if it looks like date-only (midnight) and original didn't have ':'\n",
    "                (dt.dt.hour.ne(0)) | (dt.dt.minute.ne(0)) | (dt.dt.second.ne(0)) | demands_df['Date'].astype(str).str.contains(':|T'),\n",
    "                dt.dt.date.astype(str)\n",
    "            ),\n",
    "            index=demands_df.index\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "    bulk_insert_poi_values_fast(yield_payload(), chunk=200000)\n",
    "\n",
    "    print(\"Finished bulk demand insert.\")\n",
    "\n",
    "# Restore safer settings and end transaction\n",
    "cursor.execute(\"COMMIT;\")\n",
    "cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "cursor.execute(\"PRAGMA synchronous = NORMAL;\")\n",
    "cursor.execute(\"PRAGMA journal_mode = WAL;\")\n",
    "cursor.execute(\"PRAGMA locking_mode = NORMAL;\")\n",
    "# --------------------------\n",
    "# COMMIT & CLOSE\n",
    "# --------------------------\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cabb114c5db85fd2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "561e1d36cb7b9441",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
