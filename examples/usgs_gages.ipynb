{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install geopandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:32.529860Z",
     "start_time": "2025-11-28T23:02:32.523852Z"
    }
   },
   "id": "cb88815d5ca3ce46",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "This block imports the required Python libraries:\n",
    "\n",
    "pandas for handling tabular data (such as time series).\n",
    "geopandas for working with geospatial data (GeoPackage format).\n",
    "sqlite3 for connecting and interacting with an SQLite database.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ced4d77596ac1dd5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # Pandas is used for handling tabular data efficiently\n",
    "import geopandas as gpd  # Geopandas is used for handling geospatial data\n",
    "import sqlite3  # SQLite3 is used to connect and interact with an SQLite database"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:33.587831Z",
     "start_time": "2025-11-28T23:02:33.585161Z"
    }
   },
   "id": "initial_id",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This block loads three geospatial layers from a GeoPackage (.gpkg) file using geopandas:\n",
    "\n",
    "POD_points: Contains the locations of Points of Diversion (POD), where water is withdrawn.\n",
    "event: Represents gage stations or hydrological monitoring points.\n",
    "ResOps_points: Includes information about reservoirs and their operations.\n",
    "Each layer is stored as a GeoDataFrame, allowing spatial analysis and attribute queries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1818e7e7a7181a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "data/enhanced_reference_14.gpkg: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mDataSourceError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Read geospatial layers from a GeoPackage file\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m pod_layer = \u001B[43mgpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdata/enhanced_reference_14.gpkg\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDIVERSION_POINTS\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# POD (Point of Diversion) layer contains locations where water is diverted\u001B[39;00m\n\u001B[32m      5\u001B[39m gage_layer = gpd.read_file(\u001B[33m'\u001B[39m\u001B[33mdata/enhanced_reference_14.gpkg\u001B[39m\u001B[33m'\u001B[39m, layer=\u001B[33m'\u001B[39m\u001B[33mevent\u001B[39m\u001B[33m'\u001B[39m)  \n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/geopandas/io/file.py:294\u001B[39m, in \u001B[36m_read_file\u001B[39m\u001B[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001B[39m\n\u001B[32m    291\u001B[39m             from_bytes = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    293\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m engine == \u001B[33m\"\u001B[39m\u001B[33mpyogrio\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read_file_pyogrio\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m engine == \u001B[33m\"\u001B[39m\u001B[33mfiona\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    299\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/geopandas/io/file.py:547\u001B[39m, in \u001B[36m_read_file_pyogrio\u001B[39m\u001B[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001B[39m\n\u001B[32m    538\u001B[39m     warnings.warn(\n\u001B[32m    539\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe \u001B[39m\u001B[33m'\u001B[39m\u001B[33minclude_fields\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33mignore_fields\u001B[39m\u001B[33m'\u001B[39m\u001B[33m keywords are deprecated, and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    540\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mwill be removed in a future release. You can use the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m'\u001B[39m\u001B[33m keyword \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    543\u001B[39m         stacklevel=\u001B[32m3\u001B[39m,\n\u001B[32m    544\u001B[39m     )\n\u001B[32m    545\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m] = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33minclude_fields\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m547\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpyogrio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_bytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/pyogrio/geopandas.py:261\u001B[39m, in \u001B[36mread_dataframe\u001B[39m\u001B[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001B[39m\n\u001B[32m    256\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_arrow:\n\u001B[32m    257\u001B[39m     \u001B[38;5;66;03m# For arrow, datetimes are read as is.\u001B[39;00m\n\u001B[32m    258\u001B[39m     \u001B[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001B[39;00m\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# as numpy does not directly support timezones.\u001B[39;00m\n\u001B[32m    260\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mdatetime_as_string\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m result = \u001B[43mread_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m    \u001B[49m\u001B[43mread_geometry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mread_geometry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgdal_force_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mskip_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    272\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    273\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    274\u001B[39m \u001B[43m    \u001B[49m\u001B[43msql\u001B[49m\u001B[43m=\u001B[49m\u001B[43msql\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    275\u001B[39m \u001B[43m    \u001B[49m\u001B[43msql_dialect\u001B[49m\u001B[43m=\u001B[49m\u001B[43msql_dialect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    276\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_fids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfid_as_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    277\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    280\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_arrow:\n\u001B[32m    281\u001B[39m     meta, table = result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/pyogrio/raw.py:196\u001B[39m, in \u001B[36mread\u001B[39m\u001B[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Read OGR data source into numpy arrays.\u001B[39;00m\n\u001B[32m     57\u001B[39m \n\u001B[32m     58\u001B[39m \u001B[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    191\u001B[39m \n\u001B[32m    192\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    194\u001B[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[32m--> \u001B[39m\u001B[32m196\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mogr_read\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m    \u001B[49m\u001B[43mget_vsi_path_or_buffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    201\u001B[39m \u001B[43m    \u001B[49m\u001B[43mread_geometry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mread_geometry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    202\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    203\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mskip_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    204\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    205\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    206\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    207\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_mask_to_wkb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    208\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m    \u001B[49m\u001B[43msql\u001B[49m\u001B[43m=\u001B[49m\u001B[43msql\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    210\u001B[39m \u001B[43m    \u001B[49m\u001B[43msql_dialect\u001B[49m\u001B[43m=\u001B[49m\u001B[43msql_dialect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_fids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_fids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdatetime_as_string\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdatetime_as_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/pyogrio/_io.pyx:1239\u001B[39m, in \u001B[36mpyogrio._io.ogr_read\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/Extended_hydrofabric/Hydrofabric_Extender/.venv/lib/python3.12/site-packages/pyogrio/_io.pyx:219\u001B[39m, in \u001B[36mpyogrio._io.ogr_open\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mDataSourceError\u001B[39m: data/enhanced_reference_14.gpkg: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Read geospatial layers from a GeoPackage file\n",
    "pod_layer = gpd.read_file('data/enhanced_reference_14.gpkg', layer='DIVERSION_POINTS')  \n",
    "# POD (Point of Diversion) layer contains locations where water is diverted\n",
    "\n",
    "gage_layer = gpd.read_file('data/enhanced_reference_14.gpkg', layer='event')  \n",
    "# Event layer contains gage locations or monitoring points\n",
    "\n",
    "res_layer = gpd.read_file('data/enhanced_reference_14.gpkg', layer='RESERVOIR_POINTS')  \n",
    "# Reservoir operations layer contains details about reservoirs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:34.690926Z",
     "start_time": "2025-11-28T23:02:34.631412Z"
    }
   },
   "id": "35c734f95392c613",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Config\n",
    "\n",
    "Start and end date for the simulation functionality "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a81bf64c7eb5572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# HUC4 must match what you used when building the relational DB\n",
    "huc4_code = \"14\"\n",
    "\n",
    "# Path to the relational database\n",
    "db_path = f\"data/relational_db_{huc4_code}.db\"\n",
    "\n",
    "# Simulation window for reservoir operations (change as needed)\n",
    "start_date = \"10/1/2000\"\n",
    "end_date   = \"10/1/2025\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:35.677886Z",
     "start_time": "2025-11-28T23:02:35.675589Z"
    }
   },
   "id": "e37f29254f706d75",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This block establishes a connection to an SQLite database to query key identifiers. First, it retrieves the POI_TypeID for \"USGS_Gage\" from the POI_Type table and the VariableID for \"Demand\" from the Variables table, both of which are crucial for querying related records.\n",
    "\n",
    "Next, the script iterates through each reservoir point in the res_layer, extracting its Source_comid (hydrofabric segment ID) and using it to look up the corresponding POIID in the POI table. Once the POIID is found, it queries the POI_Values table to retrieve the historical Flow data (CFS), which is then converted into a pandas.DataFrame for easy processing. Next, script retrieves data such as the POI_NativeID (a unique identifier) and the POI_Flow_ComID (hydrofabric segment ID) from the POI table.\n",
    "\n",
    "In each iteration, the script structures key variables for each point for integration into a water management model. These include the Gage unique ID, hydrofabric segment, and historical Flow records. \n",
    "\n",
    "### <span style=\"color:red\">There is a specified block in the code that can be used to implement MODEL parametrizing code.</span> \n",
    "\n",
    "Use the specified variables in each iteration to inject data to the model with model specific functions. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "457dc7b675587475"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOperationalError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m window_end   = pd.to_datetime(end_date,   \u001B[38;5;28mformat\u001B[39m=\u001B[33m\"\u001B[39m\u001B[33m%\u001B[39m\u001B[33mm/\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m/\u001B[39m\u001B[33m%\u001B[39m\u001B[33mY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Connect to the relational DB defined in the previous cell\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m conn = \u001B[43msqlite3\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdb_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m cursor = conn.cursor()\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# ------------------------------------------------------------------\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# 1. Look up the POI_TYPE_ID for USGS gages\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# ------------------------------------------------------------------\u001B[39;00m\n",
      "\u001B[31mOperationalError\u001B[39m: unable to open database file"
     ]
    }
   ],
   "source": [
    "\n",
    "window_start = pd.to_datetime(start_date, format=\"%m/%d/%Y\")\n",
    "window_end   = pd.to_datetime(end_date,   format=\"%m/%d/%Y\")\n",
    "# Connect to the relational DB defined in the previous cell\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Look up the POI_TYPE_ID for USGS gages\n",
    "# ------------------------------------------------------------------\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT POI_TYPE_ID\n",
    "    FROM POI_TYPE\n",
    "    WHERE POI_TYPE_NAME = 'USGS_GAGE'\n",
    "\"\"\")\n",
    "poi_type_result = cursor.fetchone()\n",
    "\n",
    "if poi_type_result:\n",
    "    gage_poi_type_id = poi_type_result[0]\n",
    "else:\n",
    "    conn.close()\n",
    "    raise RuntimeError(\"No POI_TYPE_ID found for 'USGS_GAGE' in the POI_TYPE table.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Look up VARIABLE_ID(s) for gage variables\n",
    "#    (change names if your VARIABLES table uses different labels)\n",
    "# ------------------------------------------------------------------\n",
    "variable_names = [\"GAGE_FLOW\"]\n",
    "variable_ids = {}\n",
    "\n",
    "for var_name in variable_names:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT VARIABLE_ID\n",
    "        FROM VARIABLES\n",
    "        WHERE VARIABLE_NAME = ?\n",
    "    \"\"\", (var_name,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        variable_ids[var_name] = result[0]\n",
    "    else:\n",
    "        print(f\"WARNING: No VARIABLE_ID found for '{var_name}' in VARIABLES\")\n",
    "\n",
    "if not variable_ids:\n",
    "    conn.close()\n",
    "    raise RuntimeError(\"No gage VARIABLE_IDs were found. Check the VARIABLES table.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Loop over gage points from the GAGE layer\n",
    "# ------------------------------------------------------------------\n",
    "for idx, row in gage_layer.iterrows():\n",
    "    # Hydrofabric COMID associated with this gage\n",
    "    # NOTE: change 'SOURCE_COMID' to match your actual column name\n",
    "    source_comid = row[\"hy_id\"]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3a. Look up POI_ID for this gage in POI table\n",
    "    # ------------------------------------------------------------------\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT POI_ID\n",
    "        FROM POI\n",
    "        WHERE POI_FLOW_COMID = ? AND POI_TYPE_ID = ?\n",
    "    \"\"\", (source_comid, gage_poi_type_id))\n",
    "    poiid_result = cursor.fetchone()\n",
    "\n",
    "    if not poiid_result:\n",
    "        # No matching POI row for this gage – skip\n",
    "        print(f\"No POI row found for gage COMID {source_comid}\")\n",
    "        continue\n",
    "\n",
    "    poiid = poiid_result[0]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3b. Get gage time-series (GAGE_FLOW) from POI_VALUES\n",
    "    # ------------------------------------------------------------------\n",
    "    timeseries_dict = {}\n",
    "\n",
    "    for var_name, var_id in variable_ids.items():\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT LOCAL_DATE_TIME, DATA_VALUE\n",
    "            FROM POI_VALUES\n",
    "            WHERE POI_ID = ? AND VARIABLE_ID = ?\n",
    "            ORDER BY LOCAL_DATE_TIME\n",
    "        \"\"\", (poiid, var_id))\n",
    "        records = cursor.fetchall()\n",
    "\n",
    "        if not records:\n",
    "            # No time series for this variable / gage\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(records, columns=[\"LocalDateTime\", var_name])\n",
    "\n",
    "# 1) Parse the datetime column\n",
    "        # If you know the exact format from the DB, be explicit; e.g. \"%Y-%m-%d %H:%M:%S\"\n",
    "        df[\"LocalDateTime\"] = pd.to_datetime(df[\"LocalDateTime\"], errors=\"coerce\")\n",
    "        \n",
    "        # 2) Quick sanity check BEFORE masking\n",
    "        print(f\"\\n=== Gage {poiid}, variable {var_name} ===\")\n",
    "        print(\"dtype:\", df[\"LocalDateTime\"].dtype)\n",
    "        print(\"min date:\", df[\"LocalDateTime\"].min())\n",
    "        print(\"max date:\", df[\"LocalDateTime\"].max())\n",
    "        \n",
    "        # If everything is NaT, parsing failed\n",
    "        if df[\"LocalDateTime\"].isna().all():\n",
    "            print(\"All LocalDateTime are NaT -> parsing problem.\")\n",
    "            continue\n",
    "        \n",
    "        # 3) Apply simulation window\n",
    "        mask = df[\"LocalDateTime\"].between(window_start, window_end, inclusive=\"both\")\n",
    "        df = df.loc[mask]\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\" -> No rows in window {window_start} to {window_end}\")\n",
    "            continue\n",
    "        \n",
    "        df.set_index(\"LocalDateTime\", inplace=True)\n",
    "        timeseries_dict[var_name] = df\n",
    "\n",
    "    if not timeseries_dict:\n",
    "        print(f\"No gage time-series found in window for COMID {source_comid}\")\n",
    "        continue\n",
    "\n",
    "    # Combine all variable DataFrames on datetime index\n",
    "    timeseries = pd.concat(timeseries_dict.values(), axis=1, join=\"outer\").reset_index()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3c. Get the gage's native ID and segment COMID from POI\n",
    "    # ------------------------------------------------------------------\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT POI_NATIVE_ID, POI_FLOW_COMID\n",
    "        FROM POI\n",
    "        WHERE POI_ID = ?\n",
    "    \"\"\", (poiid,))\n",
    "    poi_record = cursor.fetchone()\n",
    "\n",
    "    if not poi_record:\n",
    "        print(f\"No POI_NATIVE_ID/POI_FLOW_COMID found for POI_ID {poiid}\")\n",
    "        continue\n",
    "\n",
    "    poi_native_id, segment_comid = poi_record\n",
    "\n",
    "    # Optional: small sanity print\n",
    "    print(f\"Gage {idx}: POI_NATIVE_ID={poi_native_id}, segment_comid={segment_comid}\")\n",
    "    print(timeseries.head())\n",
    "\n",
    "    ##########################################################################\n",
    "    # MODEL HOOK BLOCK – use these variables for your WMM / hydrologic model\n",
    "    #\n",
    "    # 1. `POI_ID`         – Native ID of the USGS gage (POI)\n",
    "    # 2. `segment_comid`  – Hydrofabric segment ID where the gage occurs\n",
    "    # 3. `timeseries`     – Pandas DataFrame with columns:\n",
    "    #                           LocalDateTime, GAGE_FLOW\n",
    "    #\n",
    "    # -> Here is where you call your model-specific constructor / API:\n",
    "    #\n",
    "    #        gage_id          = poi_native_id,\n",
    "    #        segment_comid    = segment_comid,\n",
    "    #        ts_df            = timeseries,\n",
    "    #\n",
    "    ##########################################################################\n",
    "\n",
    "# Close DB when finished\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:36.713827Z",
     "start_time": "2025-11-28T23:02:36.681595Z"
    }
   },
   "id": "d14c96a03253d09d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gage_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# ------------------------------------------------------------------\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# SUMMARY TABLE FOR ALL GAGES\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Columns:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m \u001B[38;5;66;03m#   EndDate         – Last available datetime in DB\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# ------------------------------------------------------------------\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m gage_comids = \u001B[43mgage_layer\u001B[49m[\u001B[33m\"\u001B[39m\u001B[33mhy_id\u001B[39m\u001B[33m\"\u001B[39m].dropna().unique().tolist()\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(gage_comids) == \u001B[32m0\u001B[39m:\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo COMIDs found in gage_layer[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mhy_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'gage_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# SUMMARY TABLE FOR ALL GAGES\n",
    "# Columns:\n",
    "#   GageID          – Native POI ID of the gage\n",
    "#   SegmentCOMID    – Hydrofabric segment COMID\n",
    "#   StartDate       – First available datetime in DB\n",
    "#   EndDate         – Last available datetime in DB\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "gage_comids = gage_layer[\"hy_id\"].dropna().unique().tolist()\n",
    "\n",
    "if len(gage_comids) == 0:\n",
    "    raise RuntimeError(\"No COMIDs found in gage_layer['hy_id'].\")\n",
    "\n",
    "placeholders = \",\".join([\"?\"] * len(gage_comids))\n",
    "\n",
    "sql = f\"\"\"\n",
    "    SELECT \n",
    "        p.POI_NATIVE_ID      AS GageID,\n",
    "        p.POI_FLOW_COMID     AS SegmentCOMID,\n",
    "        MIN(v.LOCAL_DATE_TIME) AS StartDate,\n",
    "        MAX(v.LOCAL_DATE_TIME) AS EndDate\n",
    "    FROM POI p\n",
    "    JOIN POI_VALUES v\n",
    "        ON p.POI_ID = v.POI_ID\n",
    "    WHERE p.POI_TYPE_ID = ?\n",
    "      AND p.POI_FLOW_COMID IN ({placeholders})\n",
    "    GROUP BY \n",
    "        p.POI_ID,\n",
    "        p.POI_NATIVE_ID,\n",
    "        p.POI_FLOW_COMID\n",
    "    ORDER BY p.POI_FLOW_COMID\n",
    "\"\"\"\n",
    "\n",
    "params = [gage_poi_type_id] + gage_comids\n",
    "\n",
    "gage_summary_df = pd.read_sql_query(sql, conn, params=params)\n",
    "\n",
    "# Convert to datetime\n",
    "gage_summary_df[\"StartDate\"] = pd.to_datetime(gage_summary_df[\"StartDate\"], errors=\"coerce\")\n",
    "gage_summary_df[\"EndDate\"]   = pd.to_datetime(gage_summary_df[\"EndDate\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\n=== GAGE DATA SUMMARY (window-agnostic) ===\")\n",
    "print(gage_summary_df.head())\n",
    "\n",
    "output_path = f\"data/gage_table_{huc4_code}.csv\"\n",
    "gage_summary_df.to_csv(output_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-28T23:02:37.355890Z",
     "start_time": "2025-11-28T23:02:37.333433Z"
    }
   },
   "id": "80ae35b351ecc0dd",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
